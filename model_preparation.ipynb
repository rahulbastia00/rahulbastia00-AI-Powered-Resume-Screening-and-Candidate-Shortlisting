{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0986e62e-8e6f-4d04-8f28-8ebb531de324",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection & Preprocessing\n",
    "\n",
    "### Goal: \n",
    "Extract and preprocess resume and job description data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c21953d8-2188-499e-abe5-387ff0ea7956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract text from resumes\n",
    "from pdfminer.high_level import extract_text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "pdf_text = extract_text_from_pdf(\"C:/Users/rahul/OneDrive/Desktop/rahul-bastia_cvpdf.pdf\")\n",
    "# print(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69126634-22be-4024-8e4a-9b36b6580e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "def extract_text_from_docx(docx_path):\n",
    "    doc = Document(docx_path)\n",
    "    return \"\\t\".join([para.text for para in doc.paragraphs])\n",
    "docx_text = extract_text_from_docx(\"C:/Users/rahul/OneDrive/Desktop/rahul-bastia_cv.docx\")\n",
    "# print(docx_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce97b4-2426-4163-99ff-acdaa27140c8",
   "metadata": {},
   "source": [
    "# Text clening and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a20dd74-fad7-45b6-9bc8-284a1092460d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rahul bastia leetcode rahulbastia email rahul bastia00 gmail com linkedin rahulbastia phone 91 6371480952 github rahulbastia00 experience hewlett packard enterprise jan 2025 software engineering virtual intern 25 remote write proposal restful web service manage list employee build web server application java spring boot accept respond http request well support upload json datum develop run set unit test assess java spring boot application performance walmart global tech jan 2025 advanced software engineering virtual intern 25 remote solved challenge technical project various walmart team develop novel java heap datum structure shipping department demonstrate strong problem solve algorithmic skill design uml class diagram er diagram datum processing database system showcasing proficiency software design principle project job board backend node js mongodb integration mongodb express js node js code build job portal secure jwt authentication restful api use node js express mongodb enable crud operation recruiter student company manage job post profile optimize backend datum security scalability use mongoose mongodb atlas cloud storage integration efficient datum management real time ride booking system llm langchain chroma db generative ai code build full stack uber clone use microservice architecture react tailwind css socket io real time update node js express js mongodb scalable backend integrate google maps api ride tracking design modular microservice implement jwt base authentication restful apis optimize database query mongoose develop dynamic ui component state management real time socket communication content recommendation system tmdb api python vectorization code build movie recommender system python streamlit tmdb api dynamic movie poster display apply vectorization efficient content base movie similarity scoring recommendation optimize recommendation retrieval precomputed similarity matrix responsive user experience education bachelor technology gandhi institute technology gift bhubaneswar expect 2026 cgpa 4th semester 7 54 technical skill programming language python java c c javascript sql tool framework node js react js mysql mongodb github docker lang chain linux core course datum structure algorithm operating system computer network database manegement system computer architecture object orient programming machine learn devop soft skill communication team work problem solve time management certification machine learn specialization stanford deeplearning ai mern stack apna collage extracurricular achievement lead mage finding robot competition lead organizer gift intercollegiate robotic competition winner internal hackathon smart india hackathon collage level 2024 solve 280 code challenge platform like leetcode gfg practice\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "\n",
    "    # Tokenization and Lemmatization\n",
    "    doc = nlp(text)\n",
    "    cleaned_text = \" \".join([token.lemma_ for token in doc if token.text not in stopwords.words(\"english\")])\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_resume_text = preprocess_text(docx_text)\n",
    "print(cleaned_resume_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642ef36-d719-4f82-b49e-f05e1542b884",
   "metadata": {},
   "source": [
    "# Set Up MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ace66fd-95f5-4cae-b0fe-50b6d5d4efe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertOneResult(ObjectId('679baac0651a6dc1a5e406ce'), acknowledged=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "#connect to mongodb\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"resume_database\"]\n",
    "collection = db[\"resumes\"]\n",
    "\n",
    "# store processed resume\n",
    "resume_data = {\"name\": \"Candidate 1\", \"resume_text\" : cleaned_resume_text}\n",
    "collection.insert_one(resume_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383dd946",
   "metadata": {},
   "source": [
    "## Step 2: NLP-Based Resume Parsing\n",
    "\n",
    "### Goal: \n",
    "Extract key information from resumes, such as skills, education, experience, and job titles, using Named Entity Recognition (NER) and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "924c3eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Skills: Python, Java, C/C++, JavaScript, SQL, Node.js, React.js, MySQL, MongoDB, GitHub, Docker, Lang chain, Linux, Data Structure, Algorithms, Operating System, Computer Networks, Database Management System, Computer Architecture, Object-Oriented Programming, Machine Learning, DevOps, Communication, Team Work, Problem Solving, Time Management\n",
      "Extracted Experience: Software Engineering Virtual Intern at Hewlett Packard Enterprise, Advanced Software Engineering Virtual Intern at Walmart Global Tech\n",
      "Extracted Job Titles: Software Engineering Virtual Intern, Advanced Software Engineering Virtual Intern\n",
      "Extracted Education: Bachelor of Technology at Gandhi Institute For Technology (GIFT)\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# It's better to store your API key in an environment variable\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\") # Store your API key in an environment variable for security\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    "    groq_api_key=groq_api_key\n",
    ")\n",
    "\n",
    "# Make sure pdf_text is defined and contains the resume text\n",
    "pdf_text = extract_text_from_pdf(\"C:/Users/rahul/OneDrive/Desktop/rahul-bastia_cvpdf.pdf\")\n",
    "# Replace with the actual text extracted from the resume\n",
    "\n",
    "# Invoke the model to extract skills, experience, job titles, and education\n",
    "response = llm.invoke(f\"\"\"\n",
    "Here is the given text extracted from a resume: {pdf_text} \n",
    "Please extract the following information:\n",
    "- Skills\n",
    "- Experience\n",
    "- Job Titles\n",
    "- Education\n",
    "### Format the response as follows: ###\n",
    "Skills: [skill1, skill2, ...]\n",
    "Experience: [experience1, experience2, ...]\n",
    "Job Titles: [job_title1, job_title2, ...]\n",
    "Education: [degree1, degree2, ...]\n",
    "### No Preambles ###\n",
    "\"\"\")\n",
    "\n",
    "# Initialize empty strings to store skills, experience, job titles, and education\n",
    "skills_string = \"\"\n",
    "experience_string = \"\"\n",
    "job_titles_string = \"\"\n",
    "education_string = \"\"\n",
    "\n",
    "# Check if the response is valid\n",
    "if response and hasattr(response, 'content'):\n",
    "    # Assuming the response content is structured as requested\n",
    "    response_content = response.content.strip()\n",
    "    \n",
    "    # Split the response into lines\n",
    "    lines = response_content.split('\\n')\n",
    "    \n",
    "    # Extract skills, experience, job titles, and education from the response\n",
    "    for line in lines:\n",
    "        if line.startswith(\"Skills:\"):\n",
    "            skills_string = line.replace(\"Skills:\", \"\").strip()\n",
    "        elif line.startswith(\"Experience:\"):\n",
    "            experience_string = line.replace(\"Experience:\", \"\").strip()\n",
    "        elif line.startswith(\"Job Titles:\"):\n",
    "            job_titles_string = line.replace(\"Job Titles:\", \"\").strip()\n",
    "        elif line.startswith(\"Education:\"):\n",
    "            education_string = line.replace(\"Education:\", \"\").strip()\n",
    "else:\n",
    "    print(\"Error: Invalid response\")\n",
    "\n",
    "# Now you can access the extracted information as strings\n",
    "print(f\"Extracted Skills: {skills_string}\")\n",
    "print(f\"Extracted Experience: {experience_string}\")\n",
    "print(f\"Extracted Job Titles: {job_titles_string}\")\n",
    "print(f\"Extracted Education: {education_string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb608949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Software Engineering Virtual Intern, Advanced Software Engineering Virtual Intern\n"
     ]
    }
   ],
   "source": [
    "print(job_titles_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2e86f",
   "metadata": {},
   "source": [
    "# Improve Skill Extraction with Predefined Skill List\n",
    "* We will match extracted text with a predefined skill database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aecf00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted skills_string: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load predefined skill set (example list)\n",
    "predefined_skills_string = {\"Python\", \"Machine Learning\", \"Deep Learning\", \"Java\", \"Docker\", \"Kubernetes\", \"React\", \"Node.js\", \"GitHub\", \"Algorithms\"}\n",
    "\n",
    "\n",
    "def extract_skills_string(skills_string):\n",
    "    words = set(skills_string.split())\n",
    "    matched_skills_string = predefined_skills_string.intersection(words)\n",
    "    return list(matched_skills_string)\n",
    "\n",
    "extracted_skills = extract_skills_string(skills_string)\n",
    "print(\"Extracted skills_string:\", extracted_skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486fc833",
   "metadata": {},
   "source": [
    "# Convert Resumes to Embeddings for Better Matching\n",
    "* To compare resume skills with job descriptions, we use sentence-transformers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ca393eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume embedding shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the sentence-transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_resume_embedding(text):\n",
    "    return model.encode(text)\n",
    "\n",
    "resume_embedding = get_resume_embedding(skills_string)\n",
    "print(\"Resume embedding shape:\", resume_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d42dcc",
   "metadata": {},
   "source": [
    "## Store Parsed Resumes in MongoDB\n",
    "** We will store structured resume information in MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd39a430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed resume stored in MongoDB!\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"resume_database\"]\n",
    "collection = db[\"parsed_resume\"]\n",
    "\n",
    "resume_data = {\n",
    "    \"name\": \"rahul\",\n",
    "    \"education\": education_string,\n",
    "    \"experience\": experience_string,\n",
    "    \"skills\": extracted_skills,\n",
    "    \"job_title\": job_titles_string,\n",
    "    \"embedding\": resume_embedding.tolist()\n",
    "}\n",
    "collection.insert_one(resume_data)\n",
    "print(\"Parsed resume stored in MongoDB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdc4798",
   "metadata": {},
   "source": [
    "## Step 3: Candidate Shortlisting Using Machine Learning\n",
    "\n",
    "### Goal: \n",
    "Rank resumes based on job relevance using machine learning (ML). We will:\n",
    "\n",
    "- ✅ Convert job descriptions and resumes into embeddings\n",
    "- ✅ Compute similarity scores\n",
    "- ✅ Rank candidates based on job fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50c11d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job embadding shape: (384,)\n"
     ]
    }
   ],
   "source": [
    "# %pip install scikit-learn sentence-transformers numpy pandas\n",
    "# Convert Job Descriptions to Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_job_embedding(job_description):\n",
    "    return model.encode(job_description)\n",
    "\n",
    "job_description = \"We are looking for a Software Engineer with expertise in Python, Machine Learning, and Deep Learning.\"\n",
    "job_embedding = model.encode(job_description)\n",
    "\n",
    "print(\"job embadding shape:\", job_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d9872fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 resumes.\n",
      "1. rahul - Score: 0.3239\n",
      "2. rahul - Score: 0.3239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Connecting to MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"resume_database\"]\n",
    "collection = db[\"parsed_resume\"]\n",
    "\n",
    "# Retrieving Resumes\n",
    "resumes = list(collection.find())\n",
    "\n",
    "# Check if resumes are retrieved\n",
    "if not resumes:\n",
    "    print(\"No resumes found in the database.\")\n",
    "else:\n",
    "    print(f\"Found {len(resumes)} resumes.\")\n",
    "\n",
    "# Ensure job_embedding is defined\n",
    "job_description = \"We are looking for a Software Engineer with expertise in Python, Machine Learning, and Deep Learning.\"\n",
    "job_embedding = model.encode(job_description)\n",
    "\n",
    "# Computing Similarity Scores\n",
    "ranked_candidates = []\n",
    "for resume in resumes:\n",
    "    if \"embedding\" not in resume or resume[\"embedding\"] is None:\n",
    "        print(f\"Skipping {resume.get('name', 'Unknown')}: No embedding found.\")\n",
    "        continue\n",
    "\n",
    "    resume_embedding = np.array(resume[\"embedding\"]).reshape(1, -1)\n",
    "    job_embedding = np.array(job_embedding).reshape(1, -1)\n",
    "\n",
    "    # Fix typo in cosine_similarity function\n",
    "    similarity_score = cosine_similarity(resume_embedding, job_embedding)[0][0]\n",
    "    ranked_candidates.append((resume[\"name\"], similarity_score))\n",
    "\n",
    "# Sort candidates by highest similarity\n",
    "ranked_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display ranked candidates\n",
    "if not ranked_candidates:\n",
    "    print(\"No candidates matched the job description.\")\n",
    "else:\n",
    "    for rank, (name, score) in enumerate(ranked_candidates, 1):\n",
    "        print(f\"{rank}. {name} - Score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a7ccc53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortlisted candidates stored in MongoDB!\n"
     ]
    }
   ],
   "source": [
    "#  Store Shortlisting Results in MongoDB\n",
    "# Store ranking results in MongoDB\n",
    "shortlist_collection = db[\"shortlisted_candidates\"]\n",
    "\n",
    "for rank, (name, score) in enumerate(ranked_candidates, 1):\n",
    "    shortlist_collection.insert_one({\"rank\": rank, \"name\": name, \"score\": score})\n",
    "\n",
    "print(\"Shortlisted candidates stored in MongoDB!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba26fd7",
   "metadata": {},
   "source": [
    "## Step 4: Automated Feedback Generation Using LLMs\n",
    "\n",
    "### Goal: \n",
    "Generate personalized selection/rejection emails for candidates based on their ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d40c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"EMAIL_API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81f897f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 shortlisted candidates.\n"
     ]
    }
   ],
   "source": [
    "#  Retrieve Shortlisted Candidates from MongoDB\n",
    "# connect to mongodb\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"resume_database\"]\n",
    "shortlist_collection = db[\"shortlisted_candidates\"]\n",
    "\n",
    "# Fetch ranked candidates\n",
    "ranked_candidates = list(shortlist_collection.find())\n",
    "\n",
    "if not ranked_candidates:\n",
    "    print(\"No shortlisted candidates found.\")\n",
    "else:\n",
    "    print(f\"Found {len(ranked_candidates)} shortlisted candidates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fbd7398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback generated for rahul (Selected).\n",
      "Feedback generated for rahul (Selected).\n",
      "Feedback generated for rahul (Selected).\n",
      "Feedback generated for rahul (Selected).\n",
      "Feedback generated for rahul (Selected).\n",
      "Feedback generated for rahul (Selected).\n",
      "Feedback generated for rahul (Selected).\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    temperature=0,\n",
    "    groq_api_key=GROQ_API_KEY\n",
    ")\n",
    "\n",
    "def generate_feedback(name, score, threshold=0.1):\n",
    "    if score >= threshold:\n",
    "        status = \"Selected\"\n",
    "        prompt = f\"\"\"\n",
    "        Write a professional and encouraging selection email to {name}. \n",
    "        Congratulate them on their selection and mention that their skills closely match the job requirements.\n",
    "        Encourage them to prepare for the next interview round.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        status = \"Rejected\"\n",
    "        prompt = f\"\"\"\n",
    "        Write a professional rejection email to {name}. \n",
    "        Thank them for applying and mention that while their skills are valuable, they were not the best fit for this role.\n",
    "        Encourage them to apply for future opportunities.\n",
    "        \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    email_content = response.content\n",
    "    return status, email_content\n",
    "\n",
    "# Generate feedback for each candidate\n",
    "for candidate in ranked_candidates:\n",
    "    name, score = candidate[\"name\"], candidate[\"score\"]\n",
    "    status, email_content = generate_feedback(name, score)\n",
    "    \n",
    "    # Store feedback in MongoDB\n",
    "    shortlist_collection.update_one(\n",
    "        {\"name\": name},\n",
    "        {\"$set\": {\"status\": status, \"feedback\": email_content}}\n",
    "    )\n",
    "    \n",
    "    print(f\"Feedback generated for {name} ({status}).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
